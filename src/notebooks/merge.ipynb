{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a1af810-9e84-499f-af0e-3315c1d15e4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409abcd7-0fa4-494c-9848-c9eb3fb9c81b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Find right datasets in each wave\n",
    "\n",
    "In each folder associated to one wave, we have multiple datasets. However, not all datasets represent questionnaires but those are in minority. Since the goal of this notebook is to concat all questionnaires of each wave in its own dataset, we don't want to merge these data.\n",
    "\n",
    "The way we decide to define which dataset to keep or not is to define a dictionnary with wave as keys and the number of people who have fill the questionnaire as values. This way, we can *automatically* detect if a dataset is a questionnaire or not by testing if the number of rows match the number of respondants.\n",
    "\n",
    "`len(df)==wave_to_rows[f\"wave{i}\"]`\n",
    "\n",
    "**Example**\n",
    "\n",
    "The 8th wave has 46733 respondants. When iterating over each file from the wave 8 folder, we open each file, count the number of rows (`len(df)`), get the number of respondants in this wave (`wave_to_rows[f\"wave{i}\"]`) and test if these are the same values. In most cases, it's supposed to be the case (most datasets are questionnaires!) but when it's not, we just pass and go to the next dataset.\n",
    "\n",
    "**Limitations**\n",
    "\n",
    "The number of respondants is manually defined. A more scalable way to do it would have be to count the number of rows each dataset of each wave, get the *mode* (most present value) of the number of rows per wave and consider this value as the number of respondants. However, this also has some limitations (no guaranty of getting the right number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77db6ad5-2823-431a-a961-1aaee949ad6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wave_to_rows = {\n",
    "    \"wave1\": 30419,\n",
    "    \"wave2\": 37143,\n",
    "    \"wave3\": 28463,\n",
    "    \"wave4\": 58000,\n",
    "    \"wave5\": 66065,\n",
    "    \"wave6\": 68085,\n",
    "    \"wave7\": 77202,\n",
    "    \"wave8\": 46733    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46c9bb9-cb89-4f9a-9ac7-c2aa15b826bf",
   "metadata": {},
   "source": [
    "### Directory structure\n",
    "\n",
    "Directory with data: `data`.\n",
    "\n",
    "In this directory, I have multiple folders: one for each wave, unzipped. Each folder contains all the dataset for the wave. No name have been change (folders or files).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed6e5ef4-3101-4f10-ba29-dc76950dc3ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wave 1 (with 26 files) concatenated and saved!\n",
      "\n",
      "Wave 4 (with 27 files) concatenated and saved!\n",
      "\n",
      "Wave 5 (with 27 files) concatenated and saved!\n",
      "\n",
      "Wave 3 (with 15 files) concatenated and saved!\n",
      "\n",
      "Wave 6 (with 30 files) concatenated and saved!\n",
      "\n",
      "Wave 8 (with 31 files) concatenated and saved!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# iterate over each folder\n",
    "root_dir = '../../data'\n",
    "for folder in os.listdir(root_dir):\n",
    "    \n",
    "    # init parameters for the current wave\n",
    "    merged_df = pd.DataFrame()\n",
    "    nfiles = 0\n",
    "    \n",
    "    # get folder path\n",
    "    folder_path = os.path.join(root_dir, folder)\n",
    "    if os.path.isdir(folder_path) and folder != 'concat':\n",
    "        \n",
    "        # 6th element of folder name is in fact wave number\n",
    "        i = folder[6]\n",
    "\n",
    "        # iterate over each file in the folder\n",
    "        for file in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            \n",
    "            # check if it's dataset\n",
    "            if file_path.endswith('dta'):\n",
    "                \n",
    "                try:\n",
    "                    df = pd.read_stata(file_path)\n",
    "                    \n",
    "                    # test number of rows\n",
    "                    rows_expected = wave_to_rows[f\"wave{i}\"]\n",
    "                    actual_rows = len(df)\n",
    "                    if rows_expected==actual_rows:\n",
    "\n",
    "                        df = df.add_prefix(file.split('.')[0] + '_')\n",
    "\n",
    "                        # merge dfs\n",
    "                        merged_df = pd.merge(merged_df, df, left_index=True,\n",
    "                                             right_index=True, how='outer')\n",
    "                        nfiles += 1\n",
    "                \n",
    "                # avoid non-unique error\n",
    "                except ValueError:\n",
    "                    pass\n",
    "                \n",
    "        # save df\n",
    "        merged_df.to_csv(f'../../data/concat/concatwave{i}.csv.gz', index=False)\n",
    "        print(f\"Wave {i} (with {nfiles} files) concatenated and saved!\\n\")\n",
    "\n",
    "print(f\"\\n\\nTime (sec): {(time.time()-start):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
